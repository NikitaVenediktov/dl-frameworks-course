{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnDl_6-obG9f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEkj9sCobG9j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "input_size = 5\n",
        "output_size = 2\n",
        "\n",
        "batch_size = 30\n",
        "data_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp23RqwabG9k"
      },
      "source": [
        "Device\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMKMQub8bG9k"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgRj7kHSbG9l"
      },
      "outputs": [],
      "source": [
        "class RandomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, size, length):\n",
        "        self.len = length\n",
        "        self.data = torch.randn(length, size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
        "                         batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4my6ErxbG9m"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.fc(input)\n",
        "        print(\"\\tIn Model: input size\", input.size(),\n",
        "              \"output size\", output.size())\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19D-yamQbG9o"
      },
      "outputs": [],
      "source": [
        "model = Model(input_size, output_size)\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVIvKNh3bG9p"
      },
      "outputs": [],
      "source": [
        "for data in rand_loader:\n",
        "    input = data.to(device)\n",
        "    output = model(input)\n",
        "    print(\"Outside: input size\", input.size(),\n",
        "          \"output_size\", output.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYixzYSybG9p"
      },
      "source": [
        "## Results\n",
        "\n",
        "If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as\n",
        "expected. But if you have multiple GPUs, then you can get results like this.\n",
        "\n",
        "### 2 GPUs\n",
        "\n",
        "If you have 2, you will see:\n",
        "\n",
        ".. code:: bash\n",
        "\n",
        "    # on 2 GPUs\n",
        "    Let's use 2 GPUs!\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "\n",
        "### 3 GPUs\n",
        "\n",
        "If you have 3 GPUs, you will see:\n",
        "\n",
        ".. code:: bash\n",
        "\n",
        "    Let's use 3 GPUs!\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "\n",
        "### 8 GPUs\n",
        "\n",
        "If you have 8, you will see:\n",
        "\n",
        ".. code:: bash\n",
        "\n",
        "    Let's use 8 GPUs!\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install expecttest"
      ],
      "metadata": {
        "id": "HVy3KtEe1gP0",
        "outputId": "6b6f0e84-830e-4bc3-9c0d-3b1b7e30ce03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting expecttest\n",
            "  Downloading expecttest-0.1.4-py3-none-any.whl (6.5 kB)\n",
            "Installing collected packages: expecttest\n",
            "Successfully installed expecttest-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "import torch.distributed.distributed_c10d as distributed_c10d\n",
        "import torch.nn.functional as F\n",
        "from torch.distributed._shard.sharded_tensor.api import ShardedTensor\n",
        "from torch.distributed._tensor import DeviceMesh, DTensor as DT, Replicate\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp._common_utils import FSDP_WRAPPED_MODULE\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n",
        "from torch.distributed.tensor.parallel import PairwiseParallel, parallelize_module\n",
        "from torch.distributed.tensor.parallel.fsdp import enable_2d_with_fsdp\n",
        "from torch.testing._internal.common_distributed import skip_if_lt_x_gpu\n",
        "\n",
        "from torch.testing._internal.common_utils import run_tests\n",
        "\n",
        "from torch.testing._internal.distributed._tensor.common_dtensor import (\n",
        "    DTensorTestBase,\n",
        "    with_comms,\n",
        ")\n",
        "\n",
        "# Tensor-Parallel degree\n",
        "TP_DEGREE = 2\n",
        "LR = 3e-5\n",
        "\n",
        "\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net1 = torch.nn.Linear(5, 8)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.net2 = torch.nn.Linear(8, 4)\n",
        "        self.net3 = torch.nn.Linear(4, 12)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.net1(x))\n",
        "        x = F.relu(self.net2(x))\n",
        "        x = F.relu(self.net3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "def _distribute_and_fsdp_wrap_module(\n",
        "    module, module_shard, mesh_2d, fsdp_pg, use_orig_params, fsdp_nested\n",
        "):\n",
        "    if module_shard:\n",
        "        module = parallelize_module(module, mesh_2d, PairwiseParallel(), tp_mesh_dim=1)\n",
        "    pg = fsdp_pg if module_shard else distributed_c10d._get_default_group()\n",
        "\n",
        "    if fsdp_nested:\n",
        "        module.net1 = FSDP(\n",
        "            module.net1, process_group=pg, use_orig_params=use_orig_params\n",
        "        )\n",
        "        module.net2 = FSDP(\n",
        "            module.net2, process_group=pg, use_orig_params=use_orig_params\n",
        "        )\n",
        "    return FSDP(module, process_group=pg, use_orig_params=use_orig_params)\n",
        "\n",
        "\n",
        "def init_model(model_parallel_size=TP_DEGREE, use_orig_params=False, fsdp_nested=False):\n",
        "    rank = dist.get_rank()\n",
        "    torch.cuda.set_device(rank)\n",
        "    world_size = dist.get_world_size()\n",
        "\n",
        "    model = SimpleModel().cuda(rank)\n",
        "\n",
        "    # 2-D mesh is [dp, tp]\n",
        "    twod_mesh = DeviceMesh(\n",
        "        device_type=\"cuda\",\n",
        "        mesh=torch.arange(0, world_size).view(model_parallel_size, -1),\n",
        "    )\n",
        "\n",
        "    fsdp_pg = twod_mesh.get_dim_groups()[0]\n",
        "\n",
        "    # Create Input\n",
        "    model = _distribute_and_fsdp_wrap_module(\n",
        "        model, True, twod_mesh, fsdp_pg, use_orig_params, fsdp_nested\n",
        "    )\n",
        "    return model, fsdp_pg\n",
        "\n",
        "\n",
        "def is_nested_tensor(val: Any) -> bool:\n",
        "    if isinstance(val, ShardedTensor):\n",
        "        if len(val.local_shards()) == 0:\n",
        "            return False\n",
        "        if isinstance(val.local_shards()[0].tensor, ShardedTensor):\n",
        "            return True\n",
        "        if isinstance(val.local_shards()[0].tensor, DT):\n",
        "            raise ValueError(\"Cannot handle DT nested insided ST\")\n",
        "    # Safety valve for when this eventually happen\n",
        "    elif isinstance(val, DT) and isinstance(val._local_tensor, (DT, ShardedTensor)):\n",
        "        raise ValueError(\"Cannot handle nested DT\")\n",
        "    return False\n",
        "\n",
        "\n",
        "class Test2dParallelIntegration(DTensorTestBase):\n",
        "    @with_comms\n",
        "    @skip_if_lt_x_gpu(4)\n",
        "    def test_2d_fsdp_integration_functionality(self) -> None:\n",
        "        if not enable_2d_with_fsdp():\n",
        "            self.skipTest(\"FSDP 2d parallel integration not available\")\n",
        "\n",
        "        model_tp = init_model()[0]\n",
        "\n",
        "        with FSDP.state_dict_type(model_tp, StateDictType.SHARDED_STATE_DICT):\n",
        "            state_dict = model_tp.state_dict()\n",
        "            # TODO once 2D is out, validate the nesting\n",
        "            self.assertTrue(is_nested_tensor(state_dict[\"net1.weight\"]))\n",
        "            self.assertFalse(is_nested_tensor(state_dict[\"net3.bias\"]))\n",
        "\n",
        "        optim = torch.optim.Adam(model_tp.parameters(), lr=0.0001)\n",
        "\n",
        "        # Create Input\n",
        "        input_seed = self.rank\n",
        "        torch.manual_seed(input_seed + 1)\n",
        "        input = torch.rand(4, 5).cuda(self.rank)\n",
        "\n",
        "        model_tp(input).sum().backward()\n",
        "        optim.step()\n",
        "\n",
        "        optim_state = FSDP.sharded_optim_state_dict(model_tp, optim)\n",
        "        # TODO once 2D is out, validate the nesting\n",
        "        self.assertTrue(\n",
        "            is_nested_tensor(optim_state[\"state\"][\"net1.weight\"][\"exp_avg\"])\n",
        "        )\n",
        "        self.assertFalse(is_nested_tensor(optim_state[\"state\"][\"net3.bias\"][\"exp_avg\"]))\n",
        "\n",
        "    def _compare_params(self, m1, m2):\n",
        "        with FSDP.summon_full_params(m1):\n",
        "            with FSDP.summon_full_params(m2):\n",
        "                for n_p1, n_p2 in zip(m1.named_parameters(), m2.named_parameters()):\n",
        "                    p1 = n_p1[1]\n",
        "                    p2 = n_p2[1]\n",
        "                    self.assertEqual(n_p1[0], n_p2[0])\n",
        "                    name = n_p1[0]\n",
        "                    if name == \"net2.bias\" and self.rank != 0:\n",
        "                        continue\n",
        "                    if type(p2) is DT:\n",
        "                        p2 = p2.redistribute(p2.device_mesh, [Replicate()]).to_local()\n",
        "                    self.assertTrue(torch.allclose(p1, p2), f\"{p1} vs {p2}\")\n",
        "\n",
        "    def _clean_up_fsdp_param_name(self, name):\n",
        "        return \".\".join(\n",
        "            filter(lambda name: name != FSDP_WRAPPED_MODULE, name.split(\".\"))\n",
        "        )\n",
        "\n",
        "    def _test_2d_e2e_flow(\n",
        "        self, use_orig_params=False, fsdp_nested=False, multi_param_group=False\n",
        "    ) -> None:\n",
        "        if not enable_2d_with_fsdp():\n",
        "            self.skipTest(\"FSDP 2d parallel integration not available\")\n",
        "        torch.manual_seed(0)\n",
        "        model = SimpleModel().cuda(self.rank)\n",
        "        model = FSDP(model, use_orig_params=use_orig_params)\n",
        "        torch.manual_seed(0)\n",
        "        model_2d, dp_pg = init_model(\n",
        "            use_orig_params=use_orig_params, fsdp_nested=fsdp_nested\n",
        "        )\n",
        "        # Check named parameters are returning the same name at least.\n",
        "        param_names_2d = [\n",
        "            self._clean_up_fsdp_param_name(name)\n",
        "            for name, _ in model_2d.named_parameters()\n",
        "        ]\n",
        "        for name, _ in model.named_parameters():\n",
        "            name = self._clean_up_fsdp_param_name(name)\n",
        "            if name not in param_names_2d:\n",
        "                print(name, param_names_2d)\n",
        "            self.assertTrue(name in param_names_2d)\n",
        "        self._compare_params(model, model_2d)\n",
        "\n",
        "        if multi_param_group and use_orig_params:\n",
        "            param_group = [\n",
        "                {\"params\": model.net1.parameters(), \"lr\": 0.02},\n",
        "                {\"params\": model.net2.parameters(), \"lr\": 0.15},\n",
        "            ]\n",
        "            optim = torch.optim.Adam(param_group, lr=0.01)\n",
        "            param_group = [\n",
        "                {\"params\": model_2d.net1.parameters(), \"lr\": 0.02},\n",
        "                {\"params\": model_2d.net2.parameters(), \"lr\": 0.15},\n",
        "            ]\n",
        "            optim_2d = torch.optim.Adam(param_group, lr=0.01)\n",
        "        else:\n",
        "            optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "            optim_2d = torch.optim.Adam(model_2d.parameters(), lr=0.01)\n",
        "\n",
        "        for i in range(5):\n",
        "            # Ensure all input across TP ranks are same.\n",
        "            torch.manual_seed(i + dist.get_rank(dp_pg))\n",
        "            input = torch.rand(4, 5).cuda(self.rank)\n",
        "            output = model(input)\n",
        "            output_2d = model_2d(input)\n",
        "            self.assertEqual(output, output_2d)\n",
        "            output.sum().backward()\n",
        "            output_2d.sum().backward()\n",
        "            optim.step()\n",
        "            optim_2d.step()\n",
        "            self.assertEqual(model(input), model_2d(input))\n",
        "\n",
        "        # Ensure all params are still the same after optimizer update.\n",
        "        self._compare_params(model, model_2d)\n",
        "\n",
        "    @with_comms\n",
        "    @skip_if_lt_x_gpu(4)\n",
        "    def test_2d_fsdp_integration_correctness(self) -> None:\n",
        "        self._test_2d_e2e_flow()\n",
        "\n",
        "    @with_comms\n",
        "    @skip_if_lt_x_gpu(4)\n",
        "    def test_2d_fsdp_integration_use_orig_params(self) -> None:\n",
        "        self._test_2d_e2e_flow(use_orig_params=True)\n",
        "\n",
        "    @with_comms\n",
        "    @skip_if_lt_x_gpu(4)\n",
        "    def test_2d_fsdp_integration_fsdp_nested(self) -> None:\n",
        "        self._test_2d_e2e_flow(fsdp_nested=True)\n",
        "\n",
        "    @with_comms\n",
        "    @skip_if_lt_x_gpu(4)\n",
        "    def test_2d_fsdp_integration_fsdp_nested_param_groups(self) -> None:\n",
        "        self._test_2d_e2e_flow(\n",
        "            fsdp_nested=True, use_orig_params=True, multi_param_group=True\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tests(argv=['first-arg-is-ignored'])\n"
      ],
      "metadata": {
        "id": "vkiw-tOj1cIf",
        "outputId": "7e5e7686-ea0e-44aa-e4fe-5608cd9e99b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FFFFF\n",
            "======================================================================\n",
            "FAIL: test_2d_fsdp_integration_correctness (__main__.Test2dParallelIntegration)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 541, in wrapper\n",
            "    self._join_processes(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 767, in _join_processes\n",
            "    self._check_return_codes(elapsed_time)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 842, in _check_return_codes\n",
            "    self.assertEqual(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 2979, in assertEqual\n",
            "    raise error_metas[0].to_error(\n",
            "AssertionError: Scalars are not equal!\n",
            "\n",
            "Absolute difference: 1\n",
            "Relative difference: inf\n",
            "Expected zero exit code but got 1 for pid: 2532\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_2d_fsdp_integration_fsdp_nested (__main__.Test2dParallelIntegration)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 541, in wrapper\n",
            "    self._join_processes(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 767, in _join_processes\n",
            "    self._check_return_codes(elapsed_time)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 842, in _check_return_codes\n",
            "    self.assertEqual(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 2979, in assertEqual\n",
            "    raise error_metas[0].to_error(\n",
            "AssertionError: Scalars are not equal!\n",
            "\n",
            "Absolute difference: 1\n",
            "Relative difference: inf\n",
            "Expected zero exit code but got 1 for pid: 2536\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_2d_fsdp_integration_fsdp_nested_param_groups (__main__.Test2dParallelIntegration)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 541, in wrapper\n",
            "    self._join_processes(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 767, in _join_processes\n",
            "    self._check_return_codes(elapsed_time)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 842, in _check_return_codes\n",
            "    self.assertEqual(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 2979, in assertEqual\n",
            "    raise error_metas[0].to_error(\n",
            "AssertionError: Scalars are not equal!\n",
            "\n",
            "Absolute difference: 1\n",
            "Relative difference: inf\n",
            "Expected zero exit code but got 1 for pid: 2544\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_2d_fsdp_integration_functionality (__main__.Test2dParallelIntegration)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 541, in wrapper\n",
            "    self._join_processes(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 767, in _join_processes\n",
            "    self._check_return_codes(elapsed_time)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 842, in _check_return_codes\n",
            "    self.assertEqual(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 2979, in assertEqual\n",
            "    raise error_metas[0].to_error(\n",
            "AssertionError: Scalars are not equal!\n",
            "\n",
            "Absolute difference: 1\n",
            "Relative difference: inf\n",
            "Expected zero exit code but got 1 for pid: 2548\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_2d_fsdp_integration_use_orig_params (__main__.Test2dParallelIntegration)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 541, in wrapper\n",
            "    self._join_processes(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 767, in _join_processes\n",
            "    self._check_return_codes(elapsed_time)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_distributed.py\", line 842, in _check_return_codes\n",
            "    self.assertEqual(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/testing/_internal/common_utils.py\", line 2979, in assertEqual\n",
            "    raise error_metas[0].to_error(\n",
            "AssertionError: Scalars are not equal!\n",
            "\n",
            "Absolute difference: 1\n",
            "Relative difference: inf\n",
            "Expected zero exit code but got 1 for pid: 2552\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 2.767s\n",
            "\n",
            "FAILED (failures=5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}