{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install networkx\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL installation: https://www.dgl.ai/pages/start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX allows users to create, manipulate, and analyze graph structures using a variety of graph types, including directed, undirected, weighted, and bipartite graphs. It also provides a variety of algorithms for analyzing graph properties, such as centrality, clustering, connectivity, and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() \n",
    "# create a graph\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)\n",
    "# draw the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_node(1)\n",
    "G.add_nodes_from([2, 3, 4])\n",
    "G.add_edge(1, 2)\n",
    "G.add_edges_from([(1, 3), (2, 3), (4, 2)])\n",
    "# nodes and edges can be added to the graph using add_node() and add_edge() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True)\n",
    "# draw graph with node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Graph nodes: {G.nodes()}\")\n",
    "print(f\"Graph edges: {G.edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = list(G.neighbors(1))\n",
    "print(f\"Neighbors of node 1: {neighbors}\")\n",
    "# get the neighbors of a particular node using the neighbors() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.graph['name'] = 'Graph'\n",
    "print(f\"Graph attributes: {G.graph}\")\n",
    "# adding attributes to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.nodes[1]['label'] = 'Node 1'\n",
    "print(f\"Node 1 attributes: {G.nodes[1]}\")\n",
    "# adding attributes to the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges[1,3]['label'] = 'Edge 1,3'\n",
    "print(f\"Edge attributes: {G.edges[1,3]}\")\n",
    "# adding attributes to the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(G, 'graph.txt')\n",
    "# create a text file graph.txt with graph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('graph.txt')\n",
    "# import the graph from the file using read_edgelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes mean paper and edges mean citation relationships. Each node has a predefined feature with 1433 dimensions. The dataset is designed for the node classification task. The task is to predict the category of certain paper.\n",
    "\n",
    "**Statistics:**\n",
    "\n",
    "- Nodes: 2708\n",
    "- Edges: 10556\n",
    "- Number of Classes: 7\n",
    "\n",
    "**Label split:**\n",
    "\n",
    "- Train: 140\n",
    "- Valid: 500\n",
    "- Test: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CiteseerGraphDataset\n",
    "\n",
    "citeseer_dataset = CiteseerGraphDataset()\n",
    "citeseer_graph = citeseer_dataset[0]\n",
    "num_class = citeseer_dataset.num_classes\n",
    "\n",
    "# get node feature\n",
    "feat = citeseer_graph.ndata['feat']\n",
    "\n",
    "# get data split\n",
    "train_mask = citeseer_graph.ndata['train_mask']\n",
    "valid_mask = citeseer_graph.ndata['val_mask']\n",
    "test_mask = citeseer_graph.ndata['test_mask']\n",
    "\n",
    "# get labels\n",
    "label = citeseer_graph.ndata['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of nodes and their corresponding labels from the DGL graph\n",
    "node_list = citeseer_graph.nodes()\n",
    "node_labels = citeseer_graph.ndata['label']\n",
    "\n",
    "# train/test/validation\n",
    "train_node_list = node_list[train_mask]\n",
    "valid_node_list = node_list[valid_mask]\n",
    "test_node_list = node_list[test_mask]\n",
    "\n",
    "train_node_labels = node_labels[train_mask]\n",
    "valid_node_labels = node_labels[valid_mask]\n",
    "test_node_labels = node_labels[test_mask]\n",
    "\n",
    "train_feat = feat[train_mask]\n",
    "valid_feat = feat[valid_mask]\n",
    "test_feat = feat[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a graph in the NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_citeseer_graph = dgl.to_networkx(citeseer_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx_citeseer_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# create a CatBoostClassifier model\n",
    "model = CatBoostClassifier()\n",
    "\n",
    "# train the model with the train set\n",
    "model.fit(train_feat.numpy(), train_node_labels.numpy(), \n",
    "          eval_set=(valid_feat.numpy(), valid_node_labels.numpy()), \n",
    "          early_stopping_rounds=10, verbose=10)\n",
    "\n",
    "# make predictions on the test data\n",
    "predicted_labels = model.predict(test_feat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_node_labels, predicted_labels)\n",
    "classes = range(num_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Node2Vec algorithm introduced is a 2-step representation learning algorithm:\n",
    "\n",
    "- Use random walks to generate sentences from a graph. A sentence is a list of node ids. The set of all sentences makes a corpus.\n",
    "\n",
    "- The corpus is then used to learn an embedding vector for each node in the graph. Each node id is considered a unique word/token in a dictionary that has size equal to the number of nodes in the graph. The Word2Vec algorithm is used for calculating the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daled\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start training Node2Vec. We first define the parameters of the algorithm, such as the dimensionality of the embedding, the length of random walks, and the number of walks from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node2Vec parameters\n",
    "walk_length = 100\n",
    "num_walks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:05<00:00,  6.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_random_walks(graph, num_walks, walk_length, p=1.0, q=1.0):\n",
    "    walks = []\n",
    "    \n",
    "    for _ in tqdm(range(num_walks)):\n",
    "        for node in graph.nodes():\n",
    "            walk = [node]\n",
    "            for _ in range(walk_length - 1):\n",
    "                current_node = walk[-1]\n",
    "                neighbors = list(graph.neighbors(current_node))\n",
    "                \n",
    "                if len(neighbors) == 0:  # Handle nodes with no outgoing edges\n",
    "                    break\n",
    "                \n",
    "                if len(walk) == 1:  # First step in the walk\n",
    "                    walk.append(np.random.choice(neighbors))\n",
    "                else:\n",
    "                    previous_node = walk[-2]\n",
    "                    next_node = _select_next_node(graph, current_node, previous_node, neighbors, p, q)\n",
    "                    walk.append(next_node)\n",
    "                    \n",
    "            walks.append(walk)\n",
    "    \n",
    "    return walks\n",
    "\n",
    "def _select_next_node(graph, current_node, previous_node, neighbors, p, q):\n",
    "    weights = []\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor == previous_node:\n",
    "            weights.append(1 / p)\n",
    "        elif graph.has_edge(neighbor, previous_node):\n",
    "            weights.append(1)\n",
    "        else:\n",
    "            weights.append(1 / q)\n",
    "    \n",
    "    probabilities = np.array(weights) / sum(weights)\n",
    "    next_node = np.random.choice(neighbors, p=probabilities)\n",
    "    \n",
    "    return next_node\n",
    "\n",
    "\n",
    "\n",
    "# random walk generation\n",
    "walks = generate_random_walks(nx_citeseer_graph, num_walks, walk_length, p = 0.5, q = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the Word2Vec model\n",
    "model = Word2Vec(walks, vector_size = 128, window = 5, min_count = 0, sg = 1, workers = 2)\n",
    "\n",
    "# conversion of embeddings into np-array\n",
    "node_embeddings = np.array([model.wv[node_id] for node_id in range(len(nx_citeseer_graph))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = node_embeddings[train_node_list]\n",
    "valid_embeddings = node_embeddings[valid_node_list]\n",
    "test_embeddings = node_embeddings[test_node_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# сreate a CatBoost classifier with hyperparameters\n",
    "classifier = LogisticRegressionCV(Cs = 10, cv = 10, \n",
    "                                  scoring = \"accuracy\", \n",
    "                                  verbose = False,\n",
    "                                  multi_class=\"ovr\", \n",
    "                                  max_iter=200)\n",
    "\n",
    "# train the classifier on the train set\n",
    "classifier.fit(train_embeddings, train_node_labels.numpy())\n",
    "\n",
    "# predict the labels for the test set\n",
    "predicted_labels = classifier.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_node_labels, predicted_labels)\n",
    "classes = range(num_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of node2vec: https://github.com/eliorc/node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL (Deep Graph Library) is a library for deep graph learning. It provides a high-level API for creating, processing and training graph neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DGL, a graph is represented as a adjacency list consisting of two arrays: an array of nodes (ndata) and an array of edges (edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# create a graph with 3 nodes\n",
    "g = dgl.graph(([0, 1, 2], [1, 2, 0]))\n",
    "# add node features\n",
    "g.ndata['feat'] = torch.tensor([[1.0, 2.0], [2.0, 4.0], [3.0, 5.0]])\n",
    "# add edge features\n",
    "g.edata['weight'] = torch.tensor([0.5, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edata['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides various convolution operations on graphs to handle node and edge information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.randn(5, 10)\n",
    "edge_features = torch.randn(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.graph(([0, 2, 4, 3], [1, 3, 1, 0]))\n",
    "\n",
    "# add node features\n",
    "g.ndata['feat'] = node_features\n",
    "\n",
    "# add edge features\n",
    "g.edata['weight'] = edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edata['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "\n",
    "# creating a GraphConv model\n",
    "conv = dglnn.GraphConv(in_feats=10, out_feats=16)\n",
    "\n",
    "# applying convolution to a graph\n",
    "features = g.ndata['feat']\n",
    "g.ndata['h'] = conv(g, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gcn.png\" width=\"600\" height=\"240\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add self-loops\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "# applying convolution to a graph\n",
    "features = g.ndata['feat']\n",
    "g.ndata['h'] = conv(g, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.graph(([0, 2, 4, 3], [1, 3, 1, 0]))\n",
    "\n",
    "# add node features\n",
    "g.ndata['feat'] = node_features\n",
    "\n",
    "# add edge features\n",
    "g.edata['weight'] = edge_features\n",
    "\n",
    "# add reverse edges\n",
    "g = dgl.add_reverse_edges(g)\n",
    "\n",
    "# applying convolution to a graph\n",
    "features = g.ndata['feat']\n",
    "g.ndata['h'] = conv(g, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Graph Convolutional Networks (GCNs) are a type of neural network architecture specifically designed for graph-structured data. Unlike traditional neural networks that operate on grid-like data such as images or sequences, GCNs can directly handle and learn from graph data, which represents relationships between entities.\n",
    "\n",
    "The core idea behind GCNs is to generalize the convolution operation, typically used in grid-based domains, to operate on graph structures. GCNs leverage the local connectivity and graph structure to perform node feature aggregation and information propagation across the graph. This allows GCNs to capture both the local and global dependencies present in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn import GraphConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, graph, features):\n",
    "        # node features\n",
    "        graph.ndata['h'] = features\n",
    "        # aggregation \n",
    "        graph.update_all(fn.copy_u('h', 'm'), fn.mean(msg='m', out='h_neigh'))\n",
    "        # let's try to change the aggregation function(?)\n",
    "        h_neigh = graph.ndata['h_neigh']\n",
    "        \n",
    "        # linear layer\n",
    "        h = self.linear(h_neigh)\n",
    "        return h\n",
    "\n",
    "    \n",
    "# define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConvolution(in_feats, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = GraphConvolution(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = self.conv1(g, features)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(g, x)        \n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = feat.shape[1]\n",
    "hidden_size = 16\n",
    "dropout = 0.6\n",
    "\n",
    "# create model\n",
    "model_gcn = GCN(in_feats, hidden_size, num_class, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.inf\n",
    "best_model = None\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # train\n",
    "    model_gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model_gcn(citeseer_graph.to(device), feat.to(device))\n",
    "    loss = criterion(output[train_mask], train_node_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    model_gcn.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_gcn(citeseer_graph.to(device), feat.to(device))\n",
    "        val_loss = criterion(output[valid_mask], valid_node_labels.to(device))\n",
    "        \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_model = model_gcn.state_dict()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {loss:.4f} | Valid loss: {val_loss:.4f}\")\n",
    "        \n",
    "model_gcn.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    eoutput = model_gcn(citeseer_graph.to(device), feat.to(device))\n",
    "    predicted_labels = output[test_mask].max(1)[1].to('cpu')\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_node_labels, predicted_labels)\n",
    "classes = range(num_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Attention Network (GAT) is a neural network architecture designed for processing graph-structured data. It leverages attention mechanisms to enable each node in the graph to selectively attend to its neighbors during information propagation. GAT employs self-attention mechanisms to compute attention coefficients, which determine the importance of neighboring nodes for each node in the graph. By assigning different weights to different neighbors, GAT can effectively aggregate information from the relevant nodes while suppressing noise and irrelevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn import GATv2Conv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the GAT model\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_classes, num_heads, dropout):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_dim, hidden_dim, num_heads, feat_drop = 0.6, attn_drop = 0.6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = GATv2Conv(hidden_dim * num_heads, num_classes, feat_drop = 0.6, num_heads = 1, attn_drop = 0.6)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.conv1(g, features).flatten(1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(g, x).flatten(1)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = feat.shape[1]\n",
    "hidden_size = 16\n",
    "dropout = 0.6\n",
    "num_heads = 4\n",
    "\n",
    "# create model\n",
    "model_gat = GAT(in_feats, hidden_size, num_class, num_heads, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model_gat.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_gat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.inf\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # train\n",
    "    model_gat.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model_gat(citeseer_graph.to(device), feat.to(device))\n",
    "    loss = criterion(output[train_mask], train_node_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    model_gat.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_gat(citeseer_graph.to(device), feat.to(device))\n",
    "        val_loss = criterion(output[valid_mask], valid_node_labels.to(device))\n",
    "        \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_model = model_gat.state_dict()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {loss:.4f} | Valid loss: {val_loss:.4f}\")\n",
    "        \n",
    "model_gat.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_gat(citeseer_graph.to(device), feat.to(device))[test_mask]\n",
    "    predicted_labels = output.max(1)[1].to('cpu')\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_node_labels, predicted_labels)\n",
    "classes = range(num_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dgl.nn import SAGEConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, hidden_size, 'mean')\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = SAGEConv(hidden_size, num_classes, 'mean')\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = self.conv1(g, features)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(g, x)        \n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = feat.shape[1]\n",
    "hidden_size = 16\n",
    "dropout = 0.6\n",
    "\n",
    "# create model\n",
    "model_sage = GraphSAGE(in_feats, hidden_size, num_class, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model_sage.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.inf\n",
    "best_model = None\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # train\n",
    "    model_sage.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model_sage(citeseer_graph.to(device), feat.to(device))\n",
    "    loss = criterion(output[train_mask], train_node_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    model_sage.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_sage(citeseer_graph.to(device), feat.to(device))\n",
    "        val_loss = criterion(output[valid_mask], valid_node_labels.to(device))\n",
    "        \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_model = model_sage.state_dict()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {loss:.4f} | Valid loss: {val_loss:.4f}\")\n",
    "        \n",
    "model_sage.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    eoutput = model_sage(citeseer_graph.to(device), feat.to(device))\n",
    "    predicted_labels = output[test_mask].max(1)[1].to('cpu')\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(test_node_labels, predicted_labels)\n",
    "classes = range(num_class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f', xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# load the CiteSeer dataset\n",
    "dataset = Planetoid(root='data/CiteSeer', name='CiteSeer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_pyg(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, dropout):\n",
    "        super(GCN_pyg, self).__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = GCNConv(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "dropout = 0.6\n",
    "\n",
    "# create model\n",
    "model_pyg = GCN_pyg(dataset.num_features, 16, dataset.num_classes, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model_pyg.parameters(), lr = 0.0005, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.inf\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "\n",
    "# training and validation\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model_pyg.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model_pyg(dataset[0].to(device))\n",
    "    loss = criterion(output[train_mask], train_node_labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    model_pyg.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_pyg(dataset[0].to(device))\n",
    "        val_loss = criterion(output[valid_mask], valid_node_labels.to(device))\n",
    "        \n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_model = model_pyg.state_dict()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {loss:.4f} | Valid loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_pyg(dataset[0].to(device))[test_mask]\n",
    "    predicted_labels = output.max(1)[1].to('cpu')\n",
    "\n",
    "accuracy = accuracy_score(test_node_labels, predicted_labels)\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
